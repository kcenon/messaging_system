/*****************************************************************************
BSD 3-Clause License

Copyright (c) 2025, ğŸ€â˜€ğŸŒ•ğŸŒ¥ ğŸŒŠ
All rights reserved.
*****************************************************************************/

#include "optimized_storage.h"
#include <algorithm>
#include <shared_mutex>

namespace monitoring_module {

// Compressed Metrics Storage êµ¬í˜„
compressed_metrics_storage::compressed_metrics_storage(
    std::size_t capacity,
    std::chrono::steady_clock::time_point base_time)
    : base_time_(base_time)
    , capacity_(capacity)
    , storage_(std::make_unique<compressed_metric[]>(capacity)) {
}

bool compressed_metrics_storage::store(const monitoring_interface::metrics_snapshot& snapshot) {
    std::size_t index = write_index_.fetch_add(1, std::memory_order_relaxed) % capacity_;
    
    storage_[index] = compress(snapshot);
    
    std::size_t expected_size = current_size_.load(std::memory_order_relaxed);
    while (expected_size < capacity_) {
        if (current_size_.compare_exchange_weak(expected_size, expected_size + 1)) {
            break;
        }
    }
    
    return true;
}

std::optional<monitoring_interface::metrics_snapshot> 
compressed_metrics_storage::retrieve(std::size_t index) const {
    if (index >= current_size_.load(std::memory_order_acquire)) {
        return std::nullopt;
    }
    
    return decompress(storage_[index % capacity_]);
}

std::vector<monitoring_interface::metrics_snapshot> 
compressed_metrics_storage::retrieve_range(
    std::chrono::steady_clock::time_point start_time,
    std::chrono::steady_clock::time_point end_time) const {
    
    std::vector<monitoring_interface::metrics_snapshot> results;
    std::size_t size = current_size_.load(std::memory_order_acquire);
    
    for (std::size_t i = 0; i < size; ++i) {
        auto snapshot = retrieve(i);
        if (snapshot && snapshot->capture_time >= start_time && 
            snapshot->capture_time <= end_time) {
            results.push_back(*snapshot);
        }
    }
    
    return results;
}

double compressed_metrics_storage::compression_ratio() const {
    // ì›ë³¸ í¬ê¸° ëŒ€ë¹„ ì••ì¶• í¬ê¸° ë¹„ìœ¨
    constexpr std::size_t original_size = sizeof(monitoring_interface::metrics_snapshot);
    constexpr std::size_t compressed_size = sizeof(compressed_metric);
    return static_cast<double>(compressed_size) / original_size;
}

compressed_metrics_storage::compressed_metric 
compressed_metrics_storage::compress(const monitoring_interface::metrics_snapshot& snapshot) const {
    compressed_metric compressed{};
    
    // ì‹œê°„ ì••ì¶• (ì´ˆ ë‹¨ìœ„ ì˜¤í”„ì…‹)
    auto duration = snapshot.capture_time - base_time_;
    compressed.timestamp_offset = static_cast<std::uint32_t>(
        std::chrono::duration_cast<std::chrono::seconds>(duration).count());
    
    // CPU ì••ì¶• (0.01% ë‹¨ìœ„)
    compressed.cpu_percent = static_cast<std::uint16_t>(
        snapshot.system.cpu_usage_percent * 100);
    
    // ë©”ëª¨ë¦¬ ì••ì¶• (MB ë‹¨ìœ„)
    compressed.memory_mb = static_cast<std::uint32_t>(
        snapshot.system.memory_usage_bytes / (1024 * 1024));
    
    // ìŠ¤ë ˆë“œ ìˆ˜
    compressed.thread_count = static_cast<std::uint16_t>(
        snapshot.system.active_threads);
    
    // Thread pool ë©”íŠ¸ë¦­
    compressed.jobs_completed = static_cast<std::uint32_t>(
        snapshot.thread_pool.jobs_completed);
    compressed.queue_depth = static_cast<std::uint16_t>(
        snapshot.thread_pool.jobs_pending);
    compressed.latency_ms = static_cast<std::uint16_t>(
        snapshot.thread_pool.average_latency_ns / 1000000);
    
    return compressed;
}

monitoring_interface::metrics_snapshot 
compressed_metrics_storage::decompress(const compressed_metric& compressed) const {
    monitoring_interface::metrics_snapshot snapshot{};
    
    // ì‹œê°„ ë³µì›
    snapshot.capture_time = base_time_ + std::chrono::seconds(compressed.timestamp_offset);
    
    // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë³µì›
    snapshot.system.cpu_usage_percent = compressed.cpu_percent / 100;
    snapshot.system.memory_usage_bytes = 
        static_cast<std::uint64_t>(compressed.memory_mb) * 1024 * 1024;
    snapshot.system.active_threads = compressed.thread_count;
    
    // Thread pool ë©”íŠ¸ë¦­ ë³µì›
    snapshot.thread_pool.jobs_completed = compressed.jobs_completed;
    snapshot.thread_pool.jobs_pending = compressed.queue_depth;
    snapshot.thread_pool.average_latency_ns = 
        static_cast<std::uint64_t>(compressed.latency_ms) * 1000000;
    
    return snapshot;
}

// Tiered Storage êµ¬í˜„
tiered_storage::tiered_storage(std::size_t hot_capacity,
                             std::size_t warm_capacity,
                             std::size_t cold_capacity)
    : hot_tier_(hot_capacity)
    , warm_tier_(std::make_unique<compressed_metrics_storage>(
        warm_capacity, std::chrono::steady_clock::now()))
    , cold_tier_(std::make_unique<compressed_metrics_storage>(
        cold_capacity, std::chrono::steady_clock::now()))
    , last_aging_(std::chrono::steady_clock::now()) {
}

void tiered_storage::store(const monitoring_interface::metrics_snapshot& snapshot) {
    // í•« ê³„ì¸µì— ì €ì¥
    if (!hot_tier_.enqueue(snapshot)) {
        // í•« ê³„ì¸µì´ ê°€ë“ ì°¬ ê²½ìš° ì—ì´ì§• ìˆ˜í–‰
        perform_aging();
        hot_tier_.enqueue(snapshot);  // ì¬ì‹œë„
    }
}

std::optional<monitoring_interface::metrics_snapshot> 
tiered_storage::retrieve(std::chrono::steady_clock::time_point time_point) const {
    std::shared_lock<std::shared_mutex> lock(tier_mutex_);
    
    // í•« ê³„ì¸µì—ì„œ ê²€ìƒ‰ (ê°€ì¥ ìµœê·¼ ë°ì´í„°)
    // ê°„ë‹¨í•œ êµ¬í˜„ì„ ìœ„í•´ í•« ê³„ì¸µì€ ê±´ë„ˆëœ€
    
    // ì›œ ê³„ì¸µì—ì„œ ê²€ìƒ‰
    auto warm_results = warm_tier_->retrieve_range(time_point, time_point);
    if (!warm_results.empty()) {
        return warm_results.front();
    }
    
    // ì½œë“œ ê³„ì¸µì—ì„œ ê²€ìƒ‰
    auto cold_results = cold_tier_->retrieve_range(time_point, time_point);
    if (!cold_results.empty()) {
        return cold_results.front();
    }
    
    return std::nullopt;
}

void tiered_storage::perform_aging() {
    std::unique_lock<std::shared_mutex> lock(tier_mutex_);
    
    // í•« -> ì›œ ì´ë™
    monitoring_interface::metrics_snapshot snapshot;
    std::vector<monitoring_interface::metrics_snapshot> to_warm;
    
    // í•« ê³„ì¸µì˜ ì ˆë°˜ì„ ì›œìœ¼ë¡œ ì´ë™
    std::size_t to_move = hot_tier_.size() / 2;
    for (std::size_t i = 0; i < to_move; ++i) {
        if (hot_tier_.dequeue(snapshot)) {
            to_warm.push_back(snapshot);
        }
    }
    
    // ì›œ ê³„ì¸µì— ì €ì¥
    for (const auto& s : to_warm) {
        warm_tier_->store(s);
    }
    
    // ì›œ -> ì½œë“œ ì´ë™ (1ì‹œê°„ ì´ìƒ ëœ ë°ì´í„°)
    // TODO: ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì›œ ê³„ì¸µì˜ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ì½œë“œë¡œ ì´ë™
    
    last_aging_ = std::chrono::steady_clock::now();
}

tiered_storage::memory_stats tiered_storage::get_memory_stats() const {
    memory_stats stats{};
    
    // í•« ê³„ì¸µ (ì›ë³¸ í¬ê¸°)
    stats.hot_tier_bytes = hot_tier_.size() * sizeof(monitoring_interface::metrics_snapshot);
    
    // ì›œ ê³„ì¸µ (ì••ì¶•)
    stats.warm_tier_bytes = warm_tier_->memory_usage();
    
    // ì½œë“œ ê³„ì¸µ (ê³ ì••ì¶•)
    stats.cold_tier_bytes = cold_tier_->memory_usage();
    
    stats.total_bytes = stats.hot_tier_bytes + stats.warm_tier_bytes + stats.cold_tier_bytes;
    
    return stats;
}

// Batch Metrics Processor êµ¬í˜„
batch_metrics_processor::batch_metrics_processor(
    std::size_t batch_size,
    std::chrono::milliseconds flush_interval,
    batch_callback callback)
    : batch_size_(batch_size)
    , flush_interval_(flush_interval)
    , callback_(callback) {
    batch_.reserve(batch_size);
}

batch_metrics_processor::~batch_metrics_processor() {
    stop();
}

void batch_metrics_processor::add(const monitoring_interface::metrics_snapshot& snapshot) {
    std::unique_lock lock(batch_mutex_);
    
    batch_.push_back(snapshot);
    
    if (batch_.size() >= batch_size_) {
        process_batch();
    }
}

void batch_metrics_processor::flush() {
    std::unique_lock lock(batch_mutex_);
    if (!batch_.empty()) {
        process_batch();
    }
    stats_.flush_count.fetch_add(1, std::memory_order_relaxed);
}

void batch_metrics_processor::start() {
    bool expected = false;
    if (!running_.compare_exchange_strong(expected, true)) {
        return;  // ì´ë¯¸ ì‹¤í–‰ ì¤‘
    }
    
    processor_thread_ = std::thread(&batch_metrics_processor::process_loop, this);
}

void batch_metrics_processor::stop() {
    running_.store(false);
    batch_cv_.notify_all();
    
    if (processor_thread_.joinable()) {
        processor_thread_.join();
    }
    
    // ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
    flush();
}

void batch_metrics_processor::process_loop() {
    while (running_.load()) {
        std::unique_lock lock(batch_mutex_);
        
        // í”ŒëŸ¬ì‹œ ê°„ê²©ë§Œí¼ ëŒ€ê¸°
        batch_cv_.wait_for(lock, flush_interval_, [this] {
            return !running_.load() || batch_.size() >= batch_size_;
        });
        
        if (!batch_.empty()) {
            process_batch();
        }
    }
}

void batch_metrics_processor::process_batch() {
    if (batch_.empty()) {
        return;
    }
    
    // ë°°ì¹˜ ë³µì‚¬ (ì½œë°± ì‹¤í–‰ ì¤‘ ë¸”ë¡œí‚¹ ë°©ì§€)
    std::vector<monitoring_interface::metrics_snapshot> current_batch;
    batch_.swap(current_batch);
    
    // í†µê³„ ì—…ë°ì´íŠ¸
    stats_.batches_processed.fetch_add(1, std::memory_order_relaxed);
    stats_.metrics_processed.fetch_add(current_batch.size(), std::memory_order_relaxed);
    
    // ì½œë°± ì‹¤í–‰
    if (callback_) {
        callback_(current_batch);
    }
}

} // namespace monitoring_module